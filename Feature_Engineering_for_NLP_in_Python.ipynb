{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering for NLP in Python.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "90hiBGOUN2U7"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQXMrwaeN3lT"
      },
      "source": [
        "# Basic features and readability scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjLccqQ8N1FH"
      },
      "source": [
        "## Character count of Russian tweets\n",
        "In this exercise, you have been given a dataframe tweets which contains some tweets associated with Russia's Internet Research Agency and compiled by FiveThirtyEight.\n",
        "\n",
        "Your task is to create a new feature 'char_count' in tweets which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the content feature of tweets.\n",
        "\n",
        "Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvCmpMmJHIWD"
      },
      "source": [
        "link = 'https://raw.githubusercontent.com/Andikazidanef15/Feature-Engineering-for-NLP-in-Python/main/russian_tweets.csv'\n",
        "tweets = pd.read_csv(link, usecols = ['content'])[:1000]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "xM60feTuOkUF",
        "outputId": "e2c51dda-e9cf-40db-f96a-b8de0d4fc931"
      },
      "source": [
        "tweets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LIVE STREAM VIDEO=&gt; Donald Trump Rallies in Co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Muslim Attacks NYPD Cops with Meat Cleaver. Me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.@vfpatlas well that's a swella word there (di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT wehking_pamela: Bobby_Axelrod2k MMFlint don...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Жители обстреливаемых районов Донецка проводят...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>How To Inspire People With Your Music! https:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>... https://t.co/AfWdTkKQlm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Trevor Noah: Until we start treating racism li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>SenSanders: RT SenJeffMerkley: We must act bol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Police: Man arrested for February shooting at ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               content\n",
              "0    LIVE STREAM VIDEO=> Donald Trump Rallies in Co...\n",
              "1    Muslim Attacks NYPD Cops with Meat Cleaver. Me...\n",
              "2    .@vfpatlas well that's a swella word there (di...\n",
              "3    RT wehking_pamela: Bobby_Axelrod2k MMFlint don...\n",
              "4    Жители обстреливаемых районов Донецка проводят...\n",
              "..                                                 ...\n",
              "995  How To Inspire People With Your Music! https:/...\n",
              "996                        ... https://t.co/AfWdTkKQlm\n",
              "997  Trevor Noah: Until we start treating racism li...\n",
              "998  SenSanders: RT SenJeffMerkley: We must act bol...\n",
              "999  Police: Man arrested for February shooting at ...\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04EMeb5sO5X1",
        "outputId": "4410caba-8459-4f87-e0d1-35389fb54ed1"
      },
      "source": [
        "# Create a feature char_count\n",
        "tweets['char_count'] = tweets['content'].apply(lambda x: len(x))\n",
        "\n",
        "# Print the average character count\n",
        "print(tweets['char_count'].mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103.462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6awTuyizPI7D"
      },
      "source": [
        "Notice that the average character count of these tweets is approximately 104, which is much higher than the overall average tweet length of around 40 characters. Depending on what you're working on, this may be something worth investigating into. For your information, there is research that indicates that fake news articles tend to have longer titles! Therefore, even extremely basic features such as character counts can prove to be very useful in certain applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEJ_rKU2PMnT"
      },
      "source": [
        "## Word count of TED talks\n",
        "ted is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature word_count which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the transcript feature in ted.\n",
        "\n",
        "In order to complete this task, you will need to define a function count_words that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the transcript feature of ted to create the new feature word_count and compute its mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwIsi88_PJWu"
      },
      "source": [
        "link_2 = 'https://raw.githubusercontent.com/Andikazidanef15/Feature-Engineering-for-NLP-in-Python/main/ted.csv'\n",
        "ted = pd.read_csv(link_2, usecols = ['transcript'])[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "AqMipODoPd97",
        "outputId": "313b3439-0970-40bf-ecac-d64ff974a204"
      },
      "source": [
        "ted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We're going to talk — my — a new lecture, just...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a representation of your brain, and yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It's a great honor today to share with you The...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My passions are music, technology and making t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It used to be that if you wanted to get a comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Today I'm going to unpack for you three exampl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>Both myself and my brother belong to the under...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>John Hockenberry: It's great to be here with y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>What you're doing, right now, at this very mom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>We've got a real problem with math education r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            transcript\n",
              "0    We're going to talk — my — a new lecture, just...\n",
              "1    This is a representation of your brain, and yo...\n",
              "2    It's a great honor today to share with you The...\n",
              "3    My passions are music, technology and making t...\n",
              "4    It used to be that if you wanted to get a comp...\n",
              "..                                                 ...\n",
              "495  Today I'm going to unpack for you three exampl...\n",
              "496  Both myself and my brother belong to the under...\n",
              "497  John Hockenberry: It's great to be here with y...\n",
              "498  What you're doing, right now, at this very mom...\n",
              "499  We've got a real problem with math education r...\n",
              "\n",
              "[500 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blRJ3CY9Pf5U",
        "outputId": "44f5461a-0256-4b35-f842-adf86ff2de3f"
      },
      "source": [
        "# Function that returns number of words in a string\n",
        "def count_words(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "    \n",
        "    # Return the number of words\n",
        "    return len(words)\n",
        "\n",
        "# Create a new feature word_count\n",
        "ted['word_count'] = ted['transcript'].apply(count_words)\n",
        "\n",
        "# Print the average word count of the talks\n",
        "print(ted['word_count'].mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1987.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBsAI80EPw2r"
      },
      "source": [
        "Also, notice that the average length of a talk is close to 2000 words. You can use the word_count feature to compute its correlation with other variables such as number of views, number of comments, etc. and derive extremely interesting insights about TED."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "x4G8AiJjPyR1",
        "outputId": "2db53ab7-1073-4841-cbad-1094923078cb"
      },
      "source": [
        "# Function that returns numner of hashtags in a string\n",
        "def count_hashtags(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "    \n",
        "    # Create a list of words that are hashtags\n",
        "    hashtags = [word for word in words if word.startswith('#')]\n",
        "    \n",
        "    # Return number of hashtags\n",
        "    return(len(hashtags))\n",
        "\n",
        "# Create a feature hashtag_count and display distribution\n",
        "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
        "tweets['hashtag_count'].hist()\n",
        "plt.title('Hashtag count distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ZW6pNbeGQJIg",
        "outputId": "1a03bbe3-5607-4a3c-c047-d7737d9aa9ed"
      },
      "source": [
        "# Function that returns number of mentions in a string\n",
        "def count_mentions(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "    \n",
        "    # Create a list of words that are mentions\n",
        "    mentions = [word for word in words if word.startswith('@')]\n",
        "    \n",
        "    # Return number of mentions\n",
        "    return(len(mentions))\n",
        "\n",
        "# Create a feature mention_count and display distribution\n",
        "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
        "tweets['mention_count'].hist()\n",
        "plt.title('Mention count distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7vaKhcvRmjb"
      },
      "source": [
        "## Readability of 'The Myth of Sisyphus'\n",
        "In this exercise, you will compute the Flesch reading ease score for Albert Camus' famous essay The Myth of Sisyphus. We will then interpret the value of this score as explained in the video and try to determine the reading level of the essay.\n",
        "\n",
        "The entire essay is in the form of a string and is available as sisyphus_essay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnMYg9WwScAF",
        "outputId": "4c4e8522-2835-4e36-fdc7-80697a471a4b"
      },
      "source": [
        "!pip install textatistic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textatistic\n",
            "  Downloading https://files.pythonhosted.org/packages/73/f8/74dade1df8998ce9a42cba21b3cdf284f3f273e7e22fbcab27955d213e61/textatistic-0.0.1.tar.gz\n",
            "Collecting pyhyphen>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/c7/f4/9b4024552100881f63f80f4bac6e4e22a05a4c3166939bcb46f799abeeea/PyHyphen-4.0.1.tar.gz\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pyhyphen>=2.0.5->textatistic) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyhyphen>=2.0.5->textatistic) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyhyphen>=2.0.5->textatistic) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyhyphen>=2.0.5->textatistic) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyhyphen>=2.0.5->textatistic) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyhyphen>=2.0.5->textatistic) (2.10)\n",
            "Building wheels for collected packages: textatistic, pyhyphen\n",
            "  Building wheel for textatistic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textatistic: filename=textatistic-0.0.1-cp37-none-any.whl size=29069 sha256=bca4dbeccb0a07e1011ab5429884cb24ae42bfba8b0a40552891d690724e602e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/ec/34/69c3cae349149cd91552c4c470efcbd08bbd21ba30b12e08ab\n",
            "  Building wheel for pyhyphen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhyphen: filename=PyHyphen-4.0.1-cp37-abi3-linux_x86_64.whl size=52068 sha256=04dab8859c2bb1ea634c54de6cec773c3e4e542931093c24f57eacd5573bcb62\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/0c/fd/3b07283df70c067d93261f34140ab7864df6d10bfe59760adc\n",
            "Successfully built textatistic pyhyphen\n",
            "Installing collected packages: pyhyphen, textatistic\n",
            "Successfully installed pyhyphen-4.0.1 textatistic-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EJWPrF7Rnkz"
      },
      "source": [
        "sisyphus_essay = '\\nThe gods had condemned Sisyphus to ceaselessly rolling a rock to the top of a mountain, whence the stone would fall back of its own weight. They had thought with some reason that there is no more dreadful punishment than futile and hopeless labor. If one believes Homer, Sisyphus was the wisest and most prudent of mortals. According to another tradition, however, he was disposed to practice the profession of highwayman. I see no contradiction in this. Opinions differ as to the reasons why he became the futile laborer of the underworld. To begin with, he is accused of a certain levity in regard to the gods. He stole their secrets. Egina, the daughter of Esopus, was carried off by Jupiter. The father was shocked by that disappearance and complained to Sisyphus. He, who knew of the abduction, offered to tell about it on condition that Esopus would give water to the citadel of Corinth. To the celestial thunderbolts he preferred the benediction of water. He was punished for this in the underworld. Homer tells us also that Sisyphus had put Death in chains. Pluto could not endure the sight of his deserted, silent empire. He dispatched the god of war, who liberated Death from the hands of her conqueror. It is said that Sisyphus, being near to death, rashly wanted to test his wife\\'s love. He ordered her to cast his unburied body into the middle of the public square. Sisyphus woke up in the underworld. And there, annoyed by an obedience so contrary to human love, he obtained from Pluto permission to return to earth in order to chastise his wife. But when he had seen again the face of this world, enjoyed water and sun, warm stones and the sea, he no longer wanted to go back to the infernal darkness. Recalls, signs of anger, warnings were of no avail. Many years more he lived facing the curve of the gulf, the sparkling sea, and the smiles of earth. A decree of the gods was necessary. Mercury came and seized the impudent man by the collar and, snatching him from his joys, lead him forcibly back to the underworld, where his rock was ready for him. You have already grasped that Sisyphus is the absurd hero. He is, as much through his passions as through his torture. His scorn of the gods, his hatred of death, and his passion for life won him that unspeakable penalty in which the whole being is exerted toward accomplishing nothing. This is the price that must be paid for the passions of this earth. Nothing is told us about Sisyphus in the underworld. Myths are made for the imagination to breathe life into them. As for this myth, one sees merely the whole effort of a body straining to raise the huge stone, to roll it, and push it up a slope a hundred times over; one sees the face screwed up, the cheek tight against the stone, the shoulder bracing the clay-covered mass, the foot wedging it, the fresh start with arms outstretched, the wholly human security of two earth-clotted hands. At the very end of his long effort measured by skyless space and time without depth, the purpose is achieved. Then Sisyphus watches the stone rush down in a few moments toward tlower world whence he will have to push it up again toward the summit. He goes back down to the plain. It is during that return, that pause, that Sisyphus interests me. A face that toils so close to stones is already stone itself! I see that man going back down with a heavy yet measured step toward the torment of which he will never know the end. That hour like a breathing-space which returns as surely as his suffering, that is the hour of consciousness. At each of those moments when he leaves the heights and gradually sinks toward the lairs of the gods, he is superior to his fate. He is stronger than his rock. If this myth is tragic, that is because its hero is conscious. Where would his torture be, indeed, if at every step the hope of succeeding upheld him? The workman of today works everyday in his life at the same tasks, and his fate is no less absurd. But it is tragic only at the rare moments when it becomes conscious. Sisyphus, proletarian of the gods, powerless and rebellious, knows the whole extent of his wretched condition: it is what he thinks of during his descent. The lucidity that was to constitute his torture at the same time crowns his victory. There is no fate that can not be surmounted by scorn.'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of6rxoKiSaPs",
        "outputId": "a0feae3b-f6ed-4808-f2d5-c67cf44a369c"
      },
      "source": [
        "# Import Textatistic\n",
        "from textatistic import Textatistic\n",
        "\n",
        "# Compute the readability scores \n",
        "readability_scores = Textatistic(sisyphus_essay).scores\n",
        "\n",
        "# Print the flesch reading ease score\n",
        "flesch = readability_scores['flesch_score']\n",
        "print(\"The Flesch Reading Ease is %.2f\" % (flesch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Flesch Reading Ease is 81.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9M-VKnHSkg4"
      },
      "source": [
        "Notice that the score for this essay is approximately 81.67. This indicates that the essay is at the readability level of a 6th grade American student."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgnWRUjhU0pW"
      },
      "source": [
        "# Text preprocessing, POS tagging and NER\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7DWFPdU29D"
      },
      "source": [
        "## Tokenizing the Gettysburg Address\n",
        "In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.\n",
        "\n",
        "The entire speech is available as a string named gettysburg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-sgdnNfVELK"
      },
      "source": [
        "gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWlBCHYzSk7v",
        "outputId": "ae3fdcc1-a57e-471a-fbc9-07c688e69a4e"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate the tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfGtpdZuVa0a"
      },
      "source": [
        "## Lemmatizing the Gettysburg address\n",
        "In this exercise, we will perform lemmatization on the same gettysburg address from before.\n",
        "\n",
        "However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuZMHUDWVbiG",
        "outputId": "a78f9e42-079c-4eb8-cad8-2872f996f13e"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate lemmas\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Convert lemmas into a string\n",
        "print(' '.join(lemmas))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation may live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicate to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77r0pIhZVpfy"
      },
      "source": [
        "Observe the lemmatized version of the speech. It isn't very readable to humans but it is in a much more convenient format for a machine to process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNE7uTugXOwj"
      },
      "source": [
        "## Cleaning a blog post\n",
        "In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters.\n",
        "\n",
        "The excerpt is available as a string blog and has been printed to the console. The list of stopwords are available as stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3vCK6VVp11"
      },
      "source": [
        "blog = '\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uh14yr-XRF3",
        "outputId": "3acb5a0f-604a-401f-b93d-31ad5e525188"
      },
      "source": [
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load model and create Doc object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(blog)\n",
        "\n",
        "# Generate lemmatized tokens\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinging way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfkRYTb0Xwwz"
      },
      "source": [
        "Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. Also, note that the word U.S. was present in the original text. Since it had periods in between, our text cleaning process completely removed it. This may not be ideal behavior. It is always advisable to use your custom functions in place of isalpha() for more nuanced cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-CoNN5TX8Ln"
      },
      "source": [
        "## Cleaning TED talks in a dataframe\n",
        "In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe ted consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function preprocess and applying it to the transcript feature of the dataframe.\n",
        "\n",
        "The stopwords list is available as stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7YiZLF7Xxdc",
        "outputId": "3f9e29e8-4b6a-4798-9558-1c5165ed2291"
      },
      "source": [
        "# Function to preprocess text\n",
        "def preprocess(text):\n",
        "  \t# Create Doc object\n",
        "    doc = nlp(text, disable=['ner', 'parser'])\n",
        "    # Generate lemmas\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    # Remove stopwords and non-alphabetic characters\n",
        "    a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "    \n",
        "    return ' '.join(a_lemmas)\n",
        "  \n",
        "# Apply preprocess to ted['transcript']\n",
        "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
        "print(ted['transcript'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0      talk new lecture TED illusion create TED try r...\n",
            "1      representation brain brain break left half log...\n",
            "2      great honor today share Digital Universe creat...\n",
            "3      passion music technology thing combination thi...\n",
            "4      use want computer new program programming requ...\n",
            "                             ...                        \n",
            "495    today unpack example iconic design perfect sen...\n",
            "496    brother belong demographic Pat percent accord ...\n",
            "497    John Hockenberry great Tom want start question...\n",
            "498    right moment kill More car internet little mob...\n",
            "499    real problem math education right basically ha...\n",
            "Name: transcript, Length: 500, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5YnhhXMZOFO"
      },
      "source": [
        "## POS tagging in Lord of the Flies\n",
        "In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, Lord of the Flies, authored by William Golding.\n",
        "\n",
        "The passage is available as lotf and has already been printed to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlug0ZNRZOp6"
      },
      "source": [
        "lotf = 'He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIklgt_IZlIT",
        "outputId": "0a3acac1-c96b-4995-e2ad-eed8137f7446"
      },
      "source": [
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(lotf)\n",
        "\n",
        "# Generate tokens and pos tags\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NUM'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'PRON'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASE6gtvGZxK5"
      },
      "source": [
        "## Counting nouns in a piece of text\n",
        "In this exercise, we will write two functions, nouns() and proper_nouns() that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
        "\n",
        "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news.\n",
        "\n",
        "The en_core_web_sm model has already been loaded as nlp in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMDpjMwDZxv4",
        "outputId": "50e6925e-ea7e-4a10-9465-cd6bf6d5e0f8"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of proper nouns\n",
        "def proper_nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    \n",
        "    # Return number of proper nouns\n",
        "    return pos.count('PROPN')\n",
        "\n",
        "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTRNkExwZ1vw",
        "outputId": "566f113e-3711-4e6a-8f67-50f764163cdc"
      },
      "source": [
        "# Returns number of other nouns\n",
        "def nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    \n",
        "    # Return number of other nouns\n",
        "    return pos.count('NOUN')\n",
        "\n",
        "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi41yUggaFek"
      },
      "source": [
        "## Noun usage in fake news\n",
        "In this exercise, you have been given a dataframe headlines that contains news headlines that are either fake or real. Your task is to generate two new features num_propn and num_noun that represent the number of proper nouns and other nouns contained in the title feature of headlines.\n",
        "\n",
        "Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the num_propn and num_noun features in fake news detectors will improve its performance.\n",
        "\n",
        "To accomplish this task, the functions proper_nouns and nouns that you had built in the previous exercise have already been made available to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouY44cZTaHZg"
      },
      "source": [
        "link_3 = 'https://raw.githubusercontent.com/Andikazidanef15/Feature-Engineering-for-NLP-in-Python/main/fakenews.csv'\n",
        "headlines = pd.read_csv(link_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q0agJWPaTUL",
        "outputId": "d18b4cae-f16c-4529-88dc-39eaee8fe231"
      },
      "source": [
        "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
        "\n",
        "# Compute mean of proper nouns\n",
        "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
        "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
        "\n",
        "# Print results\n",
        "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean no. of proper nouns in real and fake headlines are 2.37 and 4.35 respectively\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00FQm72-apmq",
        "outputId": "09cebddc-b2e1-41df-f024-29d186ed912b"
      },
      "source": [
        "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
        "\n",
        "# Compute mean of other nouns\n",
        "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
        "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
        "\n",
        "# Print results\n",
        "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean no. of other nouns in real and fake headlines are 2.32 and 1.84 respectively\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4rTOrZEawFU"
      },
      "source": [
        "Notice how the mean number of proper nouns is considerably higher for fake news than it is for real news. The opposite seems to be true in the case of other nouns. This fact can be put to great use in designing fake news detectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDpIs--BcHYe"
      },
      "source": [
        "## Named entities in a sentence\n",
        "In this exercise, we will identify and classify the labels of various named entities in a body of text using one of spaCy's statistical models. We will also verify the veracity of these labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYpxnHTqcIgz",
        "outputId": "218af7f4-4782-439e-ddfa-fe1a65f84227"
      },
      "source": [
        "# Load the required model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc instance \n",
        "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print all named entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sundar Pichai PERSON\n",
            "Google ORG\n",
            "Mountain View GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVbEbbONcOAy"
      },
      "source": [
        "Notice how the model correctly predicted the labels of Google and Mountain View but mislabeled Sundar Pichai as an organization. As discussed in the video, the predictions of the model depend strongly on the data it is trained on. It is possible to train spaCy models on your custom data. You will learn to do this in more advanced NLP courses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxnUDKY1cbff"
      },
      "source": [
        "## Identifying people mentioned in a news article\n",
        "In this exercise, you have been given an excerpt from a news article published in TechCrunch. Your task is to write a function find_people that identifies the names of people that have been mentioned in a particular piece of text. You will then use find_people to identify the people of interest in the article.\n",
        "\n",
        "The article is available as the string tc and has been printed to the console. The required spacy model has also been already loaded as nlp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jy5ZOdVcO8J"
      },
      "source": [
        "tc = \"\\nIt’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txHDbs9Gcf1c",
        "outputId": "c2c35fe2-c30b-4e93-da50-0bd7d38c547a"
      },
      "source": [
        "def find_persons(text):\n",
        "  # Create Doc object\n",
        "  doc = nlp(text)\n",
        "  \n",
        "  # Identify the persons\n",
        "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
        "  \n",
        "  # Return persons\n",
        "  return persons\n",
        "\n",
        "print(find_persons(tc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyzXKkJW8dGX"
      },
      "source": [
        "# Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqctUSJ28ePt"
      },
      "source": [
        "## BoW model for movie reviews\n",
        "In this exercise, you have been provided with a corpus of more than 7000 movie tag lines. Your job is to generate the bag of words representation bow_matrix for these taglines. For this exercise, we will ignore the text preprocessing step and generate bow_matrix directly.\n",
        "\n",
        "We will also investigate the shape of the resultant bow_matrix. The first five taglines in corpus have been printed to the console for you to examine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w76LjgXt8g8N"
      },
      "source": [
        "link_4 = 'https://raw.githubusercontent.com/Andikazidanef15/Feature-Engineering-for-NLP-in-Python/main/movie_overviews.csv'\n",
        "corpus = pd.read_csv(link_4, usecols = ['tagline'])\n",
        "corpus.dropna(inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "whkQY77a9TG0",
        "outputId": "aed62628-d306-4ff5-b443-fe164e00dddf"
      },
      "source": [
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tagline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Roll the dice and unleash the excitement!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Friends are the people who let you be yourself...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>A Los Angeles Crime Saga</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             tagline\n",
              "1          Roll the dice and unleash the excitement!\n",
              "2  Still Yelling. Still Fighting. Still Ready for...\n",
              "3  Friends are the people who let you be yourself...\n",
              "4  Just When His World Is Back To Normal... He's ...\n",
              "5                           A Los Angeles Crime Saga"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCP8wc82812U",
        "outputId": "475a3bd4-91ee-4829-d114-417be05d94ce"
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_matrix = vectorizer.fit_transform(corpus['tagline'])\n",
        "\n",
        "# Print the shape of bow_matrix\n",
        "print(bow_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7033, 6614)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLJHoExT88J0"
      },
      "source": [
        "## Analyzing dimensionality and preprocessing\n",
        "In this exercise, you have been provided with a lem_corpus which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed.\n",
        "\n",
        "Your job is to generate the bag of words representation bow_lem_matrix for these lemmatized taglines and compare its shape with that of bow_matrix obtained in the previous exercise. The first five lemmatized taglines in lem_corpus have been printed to the console for you to examine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAz7pK-B88xA"
      },
      "source": [
        "corpus['tagline'] = corpus['tagline'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Z1u9D2LL-YVD",
        "outputId": "7f374146-0d57-47bc-b42e-cc32135e9a89"
      },
      "source": [
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tagline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>roll dice unleash excitement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yell fight ready love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>friend people let let forget</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>world Normal Surprise life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Los Angeles crime Saga</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        tagline\n",
              "1  roll dice unleash excitement\n",
              "2         yell fight ready love\n",
              "3  friend people let let forget\n",
              "4    world Normal Surprise life\n",
              "5        Los Angeles crime Saga"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gU9p4WQ-rM2",
        "outputId": "d4090094-3f2e-40fe-bdc5-e5ce2fdbcd8f"
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_lem_matrix = vectorizer.fit_transform(corpus['tagline'])\n",
        "\n",
        "# Print the shape of bow_lem_matrix\n",
        "print(bow_lem_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7033, 5212)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1HkYvn8_DL1",
        "outputId": "cd404810-973a-4c64-d9e9-b45b858a2365"
      },
      "source": [
        "# Convert bow_matrix into a DataFrame\n",
        "bow_df = pd.DataFrame(bow_lem_matrix.toarray())\n",
        "\n",
        "# Map the column names to vocabulary \n",
        "bow_df.columns = vectorizer.get_feature_names()\n",
        "\n",
        "# Print bow_df\n",
        "print(bow_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      aaargh  aaron  abandon  abby  abduction  ...  zombie  zone  zoo  zorba  zwei\n",
            "0          0      0        0     0          0  ...       0     0    0      0     0\n",
            "1          0      0        0     0          0  ...       0     0    0      0     0\n",
            "2          0      0        0     0          0  ...       0     0    0      0     0\n",
            "3          0      0        0     0          0  ...       0     0    0      0     0\n",
            "4          0      0        0     0          0  ...       0     0    0      0     0\n",
            "...      ...    ...      ...   ...        ...  ...     ...   ...  ...    ...   ...\n",
            "7028       0      0        0     0          0  ...       0     0    0      0     0\n",
            "7029       0      0        0     0          0  ...       0     0    0      0     0\n",
            "7030       0      0        0     0          0  ...       0     0    0      0     0\n",
            "7031       0      0        0     0          0  ...       0     0    0      0     0\n",
            "7032       0      0        0     0          0  ...       0     0    0      0     0\n",
            "\n",
            "[7033 rows x 5212 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_60F9zzRmziG"
      },
      "source": [
        "# TF-IDF and similarity scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L88f0cvlolmq"
      },
      "source": [
        "## tf-idf vectors for TED talks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF4nXcq_m9PE",
        "outputId": "572bf3a0-266c-4376-ac43-a40bf614bd38"
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(ted['transcript'])\n",
        "\n",
        "# Print the shape of tfidf_matrix\n",
        "print(tfidf_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 29158)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niZnzvAkoj4v"
      },
      "source": [
        "## Computing dot product\n",
        "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5815eAyIoxEO"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEs1iHPEm9WA",
        "outputId": "531897d2-abd3-4b57-d9a5-f8b8bf99d389"
      },
      "source": [
        "# Initialize numpy vectors\n",
        "A = np.array([1,3])\n",
        "B = np.array([-2,2])\n",
        "\n",
        "# Compute dot product\n",
        "dot_prod = np.dot(A, B)\n",
        "\n",
        "# Print dot product\n",
        "print(dot_prod)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmR2CXiFo1RY"
      },
      "source": [
        "## Cosine similarity matrix of a corpus\n",
        "In this exercise, you have been given a corpus, which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
        "\n",
        "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qbzGik-pBv2"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rok-fICMo10p"
      },
      "source": [
        "corpus = ['The sun is the largest celestial body in the solar system',\n",
        " 'The solar system consists of the sun and eight revolving planets',\n",
        " 'Ra was the Egyptian Sun God',\n",
        " 'The Pyramids were the pinnacle of Egyptian architecture',\n",
        " 'The quick brown fox jumps over the lazy dog']\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RngZuufzo6x4",
        "outputId": "a97b8f68-ae2a-4126-b23c-bffe701a6cb9"
      },
      "source": [
        "# Initialize an instance of tf-idf Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate the tf-idf vectors for the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Compute and print the cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(cosine_sim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
            " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
            " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
            " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
            " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FukKlT8pRGS"
      },
      "source": [
        "As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxre4r4NqYkP"
      },
      "source": [
        "## Comparing linear_kernel and cosine_similarity\n",
        "In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel.\n",
        "\n",
        "We will then compare the computation times for both functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK5mM3jfqe6q"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxQn-3mpqr4Q"
      },
      "source": [
        "### Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPc4lO60pLuQ",
        "outputId": "91dd49f8-c82b-4fe8-f166-545902cab578"
      },
      "source": [
        "# Record start time\n",
        "start = time.time()\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(cosine_sim)\n",
        "\n",
        "# Print time taken\n",
        "print(\"Time taken: %s seconds\" %(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
            " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
            " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
            " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
            " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
            "Time taken: 0.0017600059509277344 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nqSkraHquFm"
      },
      "source": [
        "### linear_kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSJ1Vljaqvtx",
        "outputId": "3f5be499-5209-4993-b464-528ede711569"
      },
      "source": [
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Record start time\n",
        "start = time.time()\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(cosine_sim)\n",
        "\n",
        "# Print time taken\n",
        "print(\"Time taken: %s seconds\" %(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
            " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
            " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
            " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
            " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
            "Time taken: 0.007668256759643555 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP0f5kCEq7Gc"
      },
      "source": [
        "Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. (NOTE: In case, you see linear_kernel taking more time, it's because the dataset we're dealing with is extremely small and Python's time module is incapable of capture such minute time differences accurately)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbhIGFk4rAxa"
      },
      "source": [
        "## Plot recommendation engine\n",
        "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you.\n",
        "\n",
        "You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
        "\n",
        "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d6ifn2ArBcy"
      },
      "source": [
        "movie_overview = pd.read_csv(link_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SET1j0rwrRkD"
      },
      "source": [
        "movie_plots = movie_overview['overview'].dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISc13EeSrXiC"
      },
      "source": [
        "# Initialize the TfidfVectorizer \n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax8FrvDKsUIK"
      },
      "source": [
        "## The recommender function\n",
        "In this exercise, we will build a recommender function get_recommendations(), as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n",
        "\n",
        "You have been given a dataset metadata that consists of the movie titles and overviews. The head of this dataset has been printed to console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x4uUQ-QsOlY"
      },
      "source": [
        "metadata = movie_overview[['title','tagline']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih2AaeRfst49"
      },
      "source": [
        "# Generate mapping between titles and index\n",
        "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
        "\n",
        "def get_recommendations(title, cosine_sim, indices):\n",
        "    # Get index of movie that matches title\n",
        "    idx = indices[title]\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    # Get the scores for 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    # Return the top 10 most similar movies\n",
        "    return metadata['title'].iloc[movie_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWJ5mdz6vHeo"
      },
      "source": [
        "## Generating word vectors\n",
        "In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as sent and has been printed to the console for your convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUk685-OvKTR"
      },
      "source": [
        "sent = 'I like apples and oranges'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix3n3aTnvIst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bddaca-1793-4cfb-9167-01a39d68643b"
      },
      "source": [
        "# Create the doc object\n",
        "doc = nlp(sent)\n",
        "\n",
        "# Compute pairwise similarity scores\n",
        "for token1 in doc:\n",
        "  for token2 in doc:\n",
        "    print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I I 1.0\n",
            "I like 0.111476846\n",
            "I apples 0.0067715086\n",
            "I and -0.117666654\n",
            "I oranges -0.0021670829\n",
            "like I 0.111476846\n",
            "like like 1.0\n",
            "like apples 0.052829094\n",
            "like and -0.15288135\n",
            "like oranges -0.043253582\n",
            "apples I 0.0067715086\n",
            "apples like 0.052829094\n",
            "apples apples 1.0\n",
            "apples and -0.0011159935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "apples oranges 0.635426\n",
            "and I -0.117666654\n",
            "and like -0.15288135\n",
            "and apples -0.0011159935\n",
            "and and 1.0\n",
            "and oranges 0.089370795\n",
            "oranges I -0.0021670829\n",
            "oranges like -0.043253582\n",
            "oranges apples 0.635426\n",
            "oranges and 0.089370795\n",
            "oranges oranges 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}